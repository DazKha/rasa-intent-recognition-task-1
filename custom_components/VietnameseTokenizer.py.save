from typing import Any, Dict, List, Text
from rasa.nlu.tokenizers.tokenizer import Token, Tokenizer
from rasa.shared.nlu.training_data.message import Message
from underthesea import word_tokenize

class VietnameseTokenizer(Tokenizer):

    provides = ["tokens"]

    def __init__(self, component_config: Dict[Text, Any] = None) -> None:
        super().__init__(component_config)

    def tokenize(self, message: Message, attribute: Text) -> List[Token]:
        text = message.get(attribute) or ""
        words = word_tokenize(text)

        tokens = []
        offset = 0
        for word in words:
            start = text.find(word, offset)
            end = start + len(word)
            tokens.append(Token(word, start))
            offset = end

        return tokens
^X




